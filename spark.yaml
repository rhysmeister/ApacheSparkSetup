---
  - hosts: all
    gather_facts: yes
    become: yes
    vars:

      apache_spark_tar_archive: http://www-us.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz

      additional_packages: [ "curl",
                             "gpgme",
                             "centos-release-scl",
                             "git",
                             "python2-pip" ]

      python_packages: [
          "numpy",
          "scipy",
          "pandas",
          "sympy",
          "nose",
          "py4j",
          "pyspark"
      ]
      # Java installation vars
      download_url: http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-8u181-linux-x64.tar.gz
      download_folder: /opt
      java_name: "{{download_folder}}/jdk1.8.0_05"
      java_archive: "{{download_folder}}/jdk-8u181-linux-x64.tar.gz"

    tasks:

      - name: Ensure wget is installed
        yum:
          name: wget
          state: present

      - name: Download Java
        command: "wget -q -O {{java_archive}} --no-check-certificate --no-cookies --header 'Cookie: oraclelicense=accept-securebackup-cookie' {{download_url}} creates={{java_archive}}"
        args:
          creates: /root/jdk.installed

      - name: Unpack archive
        command: "tar -zxf {{java_archive}} -C {{download_folder}}"
        args:
          creates: /root/jdk.installed

      - name: Fix ownership
        file: state=directory path={{java_name}} owner=root group=root recurse=yes

      - name: Make Java available for system
        shell: 'alternatives --install "/usr/bin/java" "java" "{{java_name}}/bin/java" 2000 && touch /root/jdk.installed'
        args:
          creates: /root/jdk.installed

      - name: Clean up
        file: state=absent path={{java_archive}}

      - name: Ensure epel is available
        yum:
          name: "epel-release"
          state: present

      - name: Ensure additional packages are installed
        yum:
          name: "{{ additional_packages }}"
          state: present

      - name: Download Apache Spark archive
        get_url:
          url: "{{ apache_spark_tar_archive }}"
          dest: "/home/vagrant/{{ apache_spark_tar_archive | basename }}"
        become: no

      - name:
        unarchive:
          src: "/home/vagrant/{{ apache_spark_tar_archive | basename }}"
          dest: /home/vagrant
          remote_src: yes
          creates: "/home/vagrant/{{ apache_spark_tar_archive | basename | replace('.tgz', '') }}"

      - name: Ensure /opt/apache-spark exists
        file:
          path: /opt/apache-spark
          state: directory

      - name: Move Apache files to /opt/apache-spark
        shell: "mv /home/vagrant/{{ apache_spark_tar_archive | basename | replace('.tgz', '') }}/* /opt/apache-spark/"
        args:
          creates: /opt/apache-spark/README.md

      - name: Ensure .bash_profile is correctly setup for Spark
        lineinfile:
          path: /home/vagrant/.bash_profile
          regexp: "{{ item['regexp'] }}"
          line: "{{ item['line'] }}"
        with_items:
          - { "regexp": "^export SPARK_HOME=", "line": "export SPARK_HOME=/opt/apache-spark" }
          - { "regexp": '^export PATH=\$PATH:\$SPARK_HOME/bin:/opt/jdk1.8.0_181/bin/', "line": "export PATH=$PATH:$SPARK_HOME/bin:/opt/jdk1.8.0_181/bin/" }
          - { "regexp": '^export JAVA_HOME=', "line": "export JAVA_HOME=/opt/jdk1.8.0_181/" }

      - name: Ensure  log4j.properties exists
        command: cp /opt/apache-spark/conf/log4j.properties.template /opt/apache-spark/conf/log4j.properties
        args:
          creates: /opt/apache-spark/conf/log4j.properties

      - name: Ensure logging level for Spark is set to ERROR
        lineinfile:
          path: /opt/apache-spark/conf/log4j.properties
          regexp: "^log4j.rootCategory="
          line: "log4j.rootCategory=ERROR, console"

      - name: Checkout tutorial project
        git:
          repo: https://github.com/rhysmeister/python-spark-tutorial.git
          dest: /home/vagrant/python-spark-tutorial/
        become: no
